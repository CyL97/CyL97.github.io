<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <link rel="stylesheet" href="css/css.css">
  <title>Document</title>
</head>

<body>

  <div class="header">
    <div class="nav">
      <ul>
        <li><a href="index.html">About</a> </li>
        <li><a href="Publications.html">Publications</a> </li>
        <li><a href="./CV.pdf">CV</a> </li>
      </ul>
      <div class="menu">

      </div>

    </div>
  </div>
  <div class="main">

    <div class="leader">
      <dl>
        <dt> <img src="./PersonalPhoto.jpeg" alt=""> </dt>
        <dd>
          <h1>Chaoyu Li</h1>
          <p>Ph.D. Student</p>
          <p>Arizona State University</p>
          <h3><a href="chaoyuli@asu.edu"><img src="./images/message.jpg" alt=""></a> <a href="https://github.com/CyL97"><img src="./images/in.jpg"
                alt=""></a><a href="chaoyuli@asu.edu"><img src="./images/tw.jpg" alt=""></a></h3>
        </dd>
      </dl>
    </div>



    <div class="infro">
      <p> I am a 3rd-year Ph.D. student in Computer Science at Arizona State University, advised by
        Prof. Pooyan Fazli. Before that, I got my master's degree from the University of Southern California.
      </p>
      <p> My research interests mainly lie in computer vision, especially in:</p>
      <ul>
        <li> * Multimodal Large Language Models: MLLMs in video understanding; Hallucination detection in MLLMs; Enhancing 
          video accessibility through MLLMs. </li>
        <li> * Post-Training Alignment & Robustness: Post-training methods for improving vision–language model reliability, 
          including temporal consistency modeling in video understanding, robustness against distorted or adversarial visual
          inputs, and alignment techniques for safety-critical video reasoning. </li>
        <li> * Efficient Multimodal Learning: Token- and frame-efficient training for video and multimodal models, including
          adaptive token pruning, dynamic keyframe selection, and compute-aware model design that maintains accuracy with 
          significantly reduced visual or textual input. </li>
      </ul>
    </div>

    <div class="tit">
      News
    </div>
    <div class="newslist">
      <table width="100%" border="0" cellspacing="0" cellpadding="0">
          <tr>
            <th width="100">Feb 26, 2025     </th>
            <td>Our paper <span style="font-weight: bold;">VidHalluc: Evaluating Temporal Hallucinations in Multimodal Large Language Models for Video Understanding</span> was accepted to CVPR 2025.</td>
          </tr> 
          <tr>
            <th width="100">Jan 16, 2025     </th>
            <td>Our paper <span style="font-weight: bold;">VideoA11y: Method and Dataset for Accessible Video Description</span> was accepted to CHI 2025.</td>
          </tr> 
          <tr>
            <th width="100">Aug 17, 2023     </th>
            <td>Became a Ph.D. student at ASU, advised by Prof. Pooyan Fazli.</td>
          </tr> 
          <tr>
            <th width="100">May 16, 2022     </th>
            <td>Joined USC IRIS Computer Vision Lab, advised by Prof. Ram Nevatia.</td>
          </tr> 
          <tr>
            <th width="100">
              Aug 23, 2021
            </th>
            <td> Became a master student at USC.</td>
          </tr> 
          <tr>
            <th width="100">
              May 28, 2021 
            </th>
            <td>Our paper <span style="font-weight: bold;">Infrared Action Detection in the Dark via Cross-Stream Attention Mechanism paper</span> was accepted to TMM 2021.</td>
          </tr> 

      </table>
    </div>
     
  
   
    <div class="tit">
      Selected Publications

    </div>

    <div class="selectlist">
        <dl>
          <dt>
             <img src="./images/img4.png" alt="">
          </dt>
          <dd>
            <h3>VidHalluc: Evaluating Temporal Hallucinations in Multimodal Large Language Models<br> 
              for Video Understanding</h3>
            <p>Chaoyu Li, Eun Woo Im, Pooyan Fazli.</p>
            <p> <i>  IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) ,</i> 2025</p>
            <h4>
              <a href="https://www.arxiv.org/abs/2412.03735">arXiv</a>
              <a href="https://vid-halluc.github.io/">Project</a>
            </h4>
          </dd>
        </dl>

        <dl>
          <dt>
             <img src="./images/img5.png" alt="">
          </dt>
          <dd>
            <h3>VideoA11y: Method and Dataset for Accessible Video Description</h3>
            <p>Chaoyu Li, Sid Padmanabhuni, Maryam S Cheema, Hasti Seifi, Pooyan Fazli.</p>
            <p> <i>  ACM SIGCHI Conference on Human Factors in Computing Systems (CHI) ,</i> 2025</p>
            <h4>
              <a href="https://arxiv.org/abs/2502.20480">arXiv</a>
              <a href="https://people-robots.github.io/VideoA11y/">Project</a>
            </h4>
          </dd>
        </dl>

        <dl>
          <dt>
             <img src="./images/img1.png" alt="">
          </dt>
          <dd>
            <h3> Infrared Action Detection in the Dark via Cross-Stream Attention Mechanism </h3>
            <p> Xu Chen*, Chenqiang Gao*, Chaoyu Li, Yi Yang, Deyu Meng. </p>
            <p> <i>  IEEE Transactions on Multimedia(TMM) ,</i> 2021</p>
            <h4><a href="./Infrared Action Detection in the Dark via Cross-Stream Attention Mechanism.pdf">PDF</a></h4>
          </dd>
        </dl>
        <dl>
          <dt>
             <img src="./images/img2.png" alt="">
          </dt>
          <dd>
            <h3>  NTT CQUPT@TRECVID2019 ActEV: Activities in Extended Video  </h3>
            <p> Yongqing Sun, Xu Chen, Chaoyu Li et al.  </p>
            <p> <i> TREC Video Retrieval Evaluation (TRECVID), </i> 2019</p>
            <h4><a href="./ntt_cqupt.pdf">PDF</a></h4>
          </dd>
        </dl>
        <dl>
          <dt>
             <img src="./images/img3.png" alt="">
          </dt>
          <dd>
            <h3> Face Anti-Spoofing Based on Multi-layer Domain Adaptation </h3>
            <p> Fengshun Zhou*, Chenqiang Gao*, Fang Chen, Chaoyu Li, Xindou Li, Feng Yang, Yue Zhao. </p>
            <p> <i> IEEE International Conference on Multimedia & Expo Workshops (ICMEW) , </i> 2019</p>
            <h4><a href="./FACE ANTI-SPOOFING BASED ON MULTI-LAYER DOMAIN ADAPTATION.pdf">PDF</a></h4>
          </dd>
        </dl>
    </div>
   
   
  </div>

  <div class="footer">
      <div class="footbox">
        © Copyright 2025 Chaoyu Li. Last updated: December 29, 2024.
      </div>
  </div>
  <script src="./js/jquery.min.js"></script>
  <script src="./js/js.js"></script>
</body>

</html>

